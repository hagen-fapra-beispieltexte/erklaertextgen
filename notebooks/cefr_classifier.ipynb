{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6828a3-d65f-410f-9164-028cac39d5e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 10 loss: 1.7713922142982483\n",
      "  batch 20 loss: 1.771324133872986\n",
      "  batch 30 loss: 1.6771887421607972\n",
      "  batch 40 loss: 1.5675775766372682\n",
      "  batch 50 loss: 1.4587822437286377\n",
      "  batch 60 loss: 1.3937555432319642\n",
      "LOSS train 1.3937555432319642 valid 1.2478435039520264\n",
      "EPOCH 2:\n",
      "  batch 10 loss: 1.3313370108604432\n",
      "  batch 20 loss: 1.191392707824707\n",
      "  batch 30 loss: 1.172075092792511\n",
      "  batch 40 loss: 1.186892330646515\n",
      "  batch 50 loss: 1.102652096748352\n",
      "  batch 60 loss: 1.1775652527809144\n",
      "LOSS train 1.1775652527809144 valid 1.0461429357528687\n",
      "EPOCH 3:\n",
      "  batch 10 loss: 1.096544075012207\n",
      "  batch 20 loss: 1.0057056665420532\n",
      "  batch 30 loss: 0.9921921789646149\n",
      "  batch 40 loss: 1.12746462225914\n",
      "  batch 50 loss: 1.1043359398841859\n",
      "  batch 60 loss: 1.0939607203006745\n",
      "LOSS train 1.0939607203006745 valid 1.0117326974868774\n",
      "EPOCH 4:\n",
      "  batch 10 loss: 1.0843705415725708\n",
      "  batch 20 loss: 0.9726447820663452\n",
      "  batch 30 loss: 0.9289063811302185\n",
      "  batch 40 loss: 1.0519471645355225\n",
      "  batch 50 loss: 1.0575002491474152\n",
      "  batch 60 loss: 1.0339824557304382\n",
      "LOSS train 1.0339824557304382 valid 0.9421615600585938\n",
      "EPOCH 5:\n",
      "  batch 10 loss: 0.9340535283088685\n",
      "  batch 20 loss: 0.9795798122882843\n",
      "  batch 30 loss: 0.9607609748840332\n",
      "  batch 40 loss: 1.0279940962791443\n",
      "  batch 50 loss: 0.9171883225440979\n",
      "  batch 60 loss: 0.9875382959842682\n",
      "LOSS train 0.9875382959842682 valid 0.9328096508979797\n",
      "EPOCH 6:\n",
      "  batch 10 loss: 0.937084287405014\n",
      "  batch 20 loss: 0.8955238044261933\n",
      "  batch 30 loss: 0.8510839879512787\n",
      "  batch 40 loss: 0.9253739774227142\n",
      "  batch 50 loss: 1.0359873175621033\n",
      "  batch 60 loss: 0.8915589034557343\n",
      "LOSS train 0.8915589034557343 valid 0.9071990847587585\n",
      "EPOCH 7:\n",
      "  batch 10 loss: 0.8731670498847961\n",
      "  batch 20 loss: 0.8858865559101105\n",
      "  batch 30 loss: 0.9894674599170685\n",
      "  batch 40 loss: 0.8654201924800873\n",
      "  batch 50 loss: 0.9227936506271363\n",
      "  batch 60 loss: 0.861804211139679\n",
      "LOSS train 0.861804211139679 valid 1.0483152866363525\n",
      "EPOCH 8:\n",
      "  batch 10 loss: 0.9543098926544189\n",
      "  batch 20 loss: 0.7387815654277802\n",
      "  batch 30 loss: 0.8722317636013031\n",
      "  batch 40 loss: 0.8453883349895477\n",
      "  batch 50 loss: 1.0373464584350587\n",
      "  batch 60 loss: 0.8554268777370453\n",
      "LOSS train 0.8554268777370453 valid 0.9727759957313538\n",
      "EPOCH 9:\n",
      "  batch 10 loss: 0.9219039022922516\n",
      "  batch 20 loss: 0.7999534785747529\n",
      "  batch 30 loss: 0.8363379776477814\n",
      "  batch 40 loss: 0.8358126878738403\n",
      "  batch 50 loss: 0.8942045390605926\n",
      "  batch 60 loss: 0.9063930094242096\n",
      "LOSS train 0.9063930094242096 valid 0.9438943266868591\n",
      "EPOCH 10:\n",
      "  batch 10 loss: 0.8827539503574371\n",
      "  batch 20 loss: 0.7893515467643738\n",
      "  batch 30 loss: 0.9429932296276092\n",
      "  batch 40 loss: 0.7381738185882568\n",
      "  batch 50 loss: 0.8457306385040283\n",
      "  batch 60 loss: 0.8679022490978241\n",
      "LOSS train 0.8679022490978241 valid 0.8941636681556702\n",
      "EPOCH 11:\n",
      "  batch 10 loss: 0.8860133349895477\n",
      "  batch 20 loss: 0.8440273880958558\n",
      "  batch 30 loss: 0.7974364817142486\n",
      "  batch 40 loss: 0.7481621980667115\n",
      "  batch 50 loss: 0.877041631937027\n",
      "  batch 60 loss: 0.7311643779277801\n",
      "LOSS train 0.7311643779277801 valid 0.9067651629447937\n",
      "EPOCH 12:\n",
      "  batch 10 loss: 0.8780181884765625\n",
      "  batch 20 loss: 0.8357723653316498\n",
      "  batch 30 loss: 0.7502376139163971\n",
      "  batch 40 loss: 0.7623632252216339\n",
      "  batch 50 loss: 0.7365968465805054\n",
      "  batch 60 loss: 0.8453176081180572\n",
      "LOSS train 0.8453176081180572 valid 0.9003998637199402\n",
      "EPOCH 13:\n",
      "  batch 10 loss: 0.765712034702301\n",
      "  batch 20 loss: 0.8113358914852142\n",
      "  batch 30 loss: 0.6982388436794281\n",
      "  batch 40 loss: 0.7727379858493805\n",
      "  batch 50 loss: 0.7992620766162872\n",
      "  batch 60 loss: 0.7499135732650757\n",
      "LOSS train 0.7499135732650757 valid 0.9382645487785339\n",
      "EPOCH 14:\n",
      "  batch 10 loss: 0.7334748804569244\n",
      "  batch 20 loss: 0.9007986485958099\n",
      "  batch 30 loss: 0.7029971122741699\n",
      "  batch 40 loss: 0.7697312593460083\n",
      "  batch 50 loss: 0.7847565352916718\n",
      "  batch 60 loss: 0.7735184490680694\n",
      "LOSS train 0.7735184490680694 valid 0.9398105144500732\n",
      "EPOCH 15:\n",
      "  batch 10 loss: 0.7397724509239196\n",
      "  batch 20 loss: 0.731293135881424\n",
      "  batch 30 loss: 0.7785029888153077\n",
      "  batch 40 loss: 0.8379582226276397\n",
      "  batch 50 loss: 0.7769647419452668\n",
      "  batch 60 loss: 0.7970659911632538\n",
      "LOSS train 0.7970659911632538 valid 0.9358319640159607\n",
      "EPOCH 16:\n",
      "  batch 10 loss: 0.739688777923584\n",
      "  batch 20 loss: 0.7553654074668884\n",
      "  batch 30 loss: 0.6644977271556854\n",
      "  batch 40 loss: 0.8259514927864074\n",
      "  batch 50 loss: 0.7290347874164581\n",
      "  batch 60 loss: 0.7503901571035385\n",
      "LOSS train 0.7503901571035385 valid 0.9089987874031067\n",
      "EPOCH 17:\n",
      "  batch 10 loss: 0.7030140161514282\n",
      "  batch 20 loss: 0.6777885675430297\n",
      "  batch 30 loss: 0.7795474350452423\n",
      "  batch 40 loss: 0.7409888625144958\n",
      "  batch 50 loss: 0.7986612021923065\n",
      "  batch 60 loss: 0.7531930088996888\n",
      "LOSS train 0.7531930088996888 valid 0.9312258362770081\n",
      "EPOCH 18:\n",
      "  batch 10 loss: 0.7587697565555572\n",
      "  batch 20 loss: 0.7313383638858795\n",
      "  batch 30 loss: 0.6582629919052124\n",
      "  batch 40 loss: 0.7944940626621246\n",
      "  batch 50 loss: 0.7758009850978851\n",
      "  batch 60 loss: 0.7056178212165832\n",
      "LOSS train 0.7056178212165832 valid 0.9411678314208984\n",
      "EPOCH 19:\n",
      "  batch 10 loss: 0.8208993703126908\n",
      "  batch 20 loss: 0.7258542120456696\n",
      "  batch 30 loss: 0.7402124047279358\n",
      "  batch 40 loss: 0.6718525767326355\n",
      "  batch 50 loss: 0.6930309623479843\n",
      "  batch 60 loss: 0.7080464124679565\n",
      "LOSS train 0.7080464124679565 valid 0.9293428659439087\n",
      "EPOCH 20:\n",
      "  batch 10 loss: 0.7254009336233139\n",
      "  batch 20 loss: 0.7582934677600861\n",
      "  batch 30 loss: 0.691259640455246\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    131\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 132\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    135\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[7], line 114\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(epoch_idx)\u001b[0m\n\u001b[1;32m    111\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    112\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 114\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m9\u001b[39m:\n\u001b[1;32m    117\u001b[0m     last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import jsonlines\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, Subset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "DATASET_PATH = \"kaggle_preprocessed.jsonl\"\n",
    "MAX_LENGTH = 512\n",
    "N_FEATURES = 28\n",
    "\n",
    "LEVELS = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]\n",
    "\n",
    "class SimpleWikiDataset(Dataset):\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = tokenizer\n",
    "        self.texts = list(jsonlines.open(DATASET_PATH).iter())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokenized = tokenizer(self.texts[index][\"text\"],\n",
    "                            return_tensors='pt',\n",
    "                            padding='max_length', max_length=self.max_length,\n",
    "                           truncation=True)\n",
    "        inputs = {}\n",
    "        inputs[\"input_ids\"] = torch.squeeze(tokenized[\"input_ids\"])\n",
    "        inputs[\"attention_mask\"] = torch.squeeze(tokenized[\"attention_mask\"])\n",
    "        inputs[\"features\"] = torch.tensor(self.texts[index][\"features\"])\n",
    "        cefr_label = torch.tensor(LEVELS.index(self.texts[index][\"label\"]))\n",
    "\n",
    "        return inputs, cefr_label\n",
    "\n",
    "class CEFRClassifier(nn.Module):\n",
    "    def __init__(self, num_cefr_levels):\n",
    "        super(CEFRClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "        # Freeze distilBERT params\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.fc1 = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 128)\n",
    "        self.fc3 = nn.Linear(28, 28)\n",
    "        self.output = nn.Linear(128 + 28, num_cefr_levels)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, aux_features):\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_output.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0]\n",
    "\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        y = self.fc3(aux_features)\n",
    "        y = self.relu(y)\n",
    "        y = self.dropout(y)\n",
    "        \n",
    "        return self.output(torch.cat((x,y), -1))\n",
    "\n",
    "device = \"cuda\"\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "dataset = SimpleWikiDataset(tokenizer, MAX_LENGTH)\n",
    "\n",
    "model = CEFRClassifier(len(LEVELS))\n",
    "model.to(device)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_set, validation_set, test_set = torch.utils.data.random_split(dataset, [0.7,0.2,0.1], generator=generator)\n",
    "\n",
    "bs = 16\n",
    "\n",
    "training_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "def train_step(epoch_idx):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, train_data in enumerate(training_loader):\n",
    "        train_inputs, train_labels = train_data\n",
    "\n",
    "        input_ids = train_inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = train_inputs[\"attention_mask\"].to(device)\n",
    "        aux_features = train_inputs[\"features\"].to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(input_ids, attention_mask, aux_features)\n",
    "        loss = loss_fn(logits, train_labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 10 == 9:\n",
    "            last_loss = running_loss / 10\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_idx * len(training_loader) + i + 1\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "EPOCHS = 30\n",
    "best_vloss = 1_000_000.0\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_step(epoch)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, validation_data in enumerate(validation_loader):\n",
    "            validation_inputs, validation_labels = validation_data\n",
    "\n",
    "            input_ids = validation_inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = validation_inputs[\"attention_mask\"].to(device)\n",
    "            aux_features = validation_inputs[\"features\"].to(device)\n",
    "            validation_labels = validation_labels.to(device)\n",
    "         \n",
    "            validation_logits = model(input_ids, attention_mask, aux_features)\n",
    "            validation_loss = loss_fn(validation_logits, validation_labels)\n",
    "                \n",
    "            running_vloss += validation_loss\n",
    "\n",
    "    avg_vloss = running_vloss / len(validation_loader)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'runs/model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7c8bc7a-a163-4dff-805d-567b1ce626df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.8031296133995056\n",
      "Validation Accuracy per 100 steps: 68.75\n",
      "Validation Loss Epoch: 0.838558030128479\n",
      "Validation Accuracy Epoch: 66.44295302013423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "66.44295302013423"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_model = CEFRClassifier(len(LEVELS))\n",
    "eval_model.load_state_dict(torch.load(\"runs/model_20240712_092923_9\"))\n",
    "\n",
    "bs = 16\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=True)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def compute_accuracy(big_idx, targets):\n",
    "    return (big_idx==targets).sum().item()\n",
    "\n",
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total_loss = 0\n",
    "    nb_tr_examples = 0\n",
    "    nb_tr_steps = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testing_loader):\n",
    "            test_inputs, test_labels = data\n",
    "\n",
    "            ids = test_inputs['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = test_inputs['attention_mask'].to(device, dtype = torch.long)\n",
    "            aux_features = test_inputs[\"features\"].to(device)\n",
    "            targets = test_labels.to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask, aux_features).squeeze()\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += compute_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            if i%100==0:\n",
    "                loss_step = total_loss/(i+1)\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "                \n",
    "    epoch_loss = total_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n",
    "\n",
    "validate(eval_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73615a-b180-44b0-bb2b-bbe15f5fc8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
