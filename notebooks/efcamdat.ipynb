{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad404688-1c9b-4980-bd5e-455577753798",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 17:25:14.574529: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-08 17:25:14.574588: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-08 17:25:14.575984: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-08 17:25:14.583902: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-08 17:25:15.489794: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b518331c-1385-4b89-ad9a-e542237d1afe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EFCamDatSet(Dataset):\n",
    "    def __init__(self, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.efcamdat = pd.read_csv(\"./efcamdat_cleaned2.csv\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.efcamdat)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cefr_numeric = self.efcamdat.loc[index, \"cefr_numeric\"]\n",
    "        text = self.efcamdat.loc[index, \"text\"]\n",
    "        text_corrected = self.efcamdat.loc[index, \"text_corrected\"]\n",
    "\n",
    "        #print(cefr_numeric)\n",
    "        #print(text_corrected)\n",
    "        tokenized = tokenizer(text_corrected,\n",
    "                            return_tensors='pt',\n",
    "                            padding='max_length', max_length=max_length,\n",
    "                           truncation=True)\n",
    "        inputs = {}\n",
    "        inputs[\"input_ids\"] = torch.squeeze(torch.tensor(tokenized[\"input_ids\"], dtype=torch.long))\n",
    "        inputs[\"attention_mask\"] = torch.squeeze(torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long))\n",
    "        cefr_level = torch.tensor(cefr_numeric - 1)\n",
    "\n",
    "        return inputs, cefr_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59956bbb-b711-47d7-8410-c1070781ca00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CEFRClassifier(nn.Module):\n",
    "    def __init__(self, num_cefr_levels):\n",
    "        super(CEFRClassifier, self).__init__()\n",
    "        \n",
    "        self.bert = AutoModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "                                             #torch_dtype=torch.float16,\n",
    "                                             #attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "        # Freeze distilBERT params\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "        # Classifier layers\n",
    "        self.pre_classifier = nn.Linear(768, 768)\n",
    "        self.fc2 = nn.Linear(768, 128)\n",
    "        self.output = nn.Linear(128, num_cefr_levels)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.softmax = nn.Softmax()\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = output.last_hidden_state\n",
    "        pooled_output = sequence_output[:, 0]\n",
    "\n",
    "        #outputs_text_corrected = self.bert(input_ids=input_ids[:,1], attention_mask=attention_mask[:,1])\n",
    "        #sequence_outputs_text_corrected = outputs_text_corrected.last_hidden_state\n",
    "        #pooled_outputs_text_corrected = sequence_outputs_text_corrected[:, 0]\n",
    "\n",
    "        #combined_output = torch.cat((pooled_outputs_text, pooled_outputs_text_corrected), dim=1)\n",
    "    \n",
    "        x = self.pre_classifier(pooled_output)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        logits = self.output(x)\n",
    "        # probs = self.softmax(logits)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015a42a8-2bdc-48af-aba9-33e6c046fe2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0157094486254795b60e1ce7c210d3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e7285535dc485cb4483918efa23106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc1a203366694cf2a142d02fc88451dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "397de3a953df4ac7845a653fdec05474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e87b677ef54ff5929e1a73872cc5aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "max_length = 450\n",
    "# max_length = 512\n",
    "dataset = EFCamDatSet(tokenizer, max_length)\n",
    "\n",
    "num_cefr_levels = 5\n",
    "num_samples = 100\n",
    "model = CEFRClassifier(num_cefr_levels)\n",
    "model.to(device)\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "#sampled_indices = torch.randperm(len(dataset), generator=generator)[:num_samples]\n",
    "#dataset = Subset(dataset, sampled_indices.tolist())\n",
    "train_set, validation_set, test_set = torch.utils.data.random_split(dataset, [0.7,0.2,0.1], generator=generator)\n",
    "\n",
    "bs = 32\n",
    "\n",
    "training_loader = DataLoader(train_set, batch_size=bs, shuffle=True)\n",
    "validation_loader = DataLoader(validation_set, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def train_step(epoch_idx, writer):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for i, train_data in enumerate(training_loader):\n",
    "        train_inputs, train_labels = train_data\n",
    "\n",
    "        input_ids = train_inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = train_inputs[\"attention_mask\"].to(device)\n",
    "        train_labels = train_labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass and loss computation\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = loss_fn(logits, train_labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 100 == 99:\n",
    "            last_loss = running_loss / 100\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_idx * len(training_loader) + i + 1\n",
    "            writer.add_scalar('Loss/train', last_loss, tb_x)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return last_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d748e4-824d-45f9-807a-fbd334c49ae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_593/4153173265.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs[\"input_ids\"] = torch.squeeze(torch.tensor(tokenized[\"input_ids\"], dtype=torch.long))\n",
      "/tmp/ipykernel_593/4153173265.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs[\"attention_mask\"] = torch.squeeze(torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  batch 100 loss: 0.9038429141044617\n",
      "  batch 200 loss: 0.5141716095805168\n",
      "  batch 300 loss: 0.4408052898943424\n",
      "  batch 400 loss: 0.39013179406523707\n",
      "  batch 500 loss: 0.33955736495554445\n",
      "  batch 600 loss: 0.2985284306108952\n",
      "  batch 700 loss: 0.31322404034435747\n",
      "  batch 800 loss: 0.2664891039580107\n",
      "  batch 900 loss: 0.27265708826482293\n",
      "  batch 1000 loss: 0.2661874966323376\n",
      "  batch 1100 loss: 0.22351632364094257\n",
      "  batch 1200 loss: 0.2176860886067152\n",
      "  batch 1300 loss: 0.21116969250142575\n",
      "  batch 1400 loss: 0.20908818699419499\n",
      "  batch 1500 loss: 0.18641429483890534\n",
      "  batch 1600 loss: 0.1942133314907551\n",
      "  batch 1700 loss: 0.18957546710968018\n",
      "  batch 1800 loss: 0.1732452769204974\n",
      "  batch 1900 loss: 0.1733451260626316\n",
      "  batch 2000 loss: 0.18112654656171798\n",
      "  batch 2100 loss: 0.16530756156891585\n",
      "  batch 2200 loss: 0.19407229084521532\n",
      "  batch 2300 loss: 0.17308582104742526\n",
      "  batch 2400 loss: 0.1609939555823803\n",
      "  batch 2500 loss: 0.16635374892503024\n",
      "  batch 2600 loss: 0.15788882218301295\n",
      "  batch 2700 loss: 0.1392459886521101\n",
      "  batch 2800 loss: 0.15165829803794623\n",
      "  batch 2900 loss: 0.14808693524450064\n",
      "  batch 3000 loss: 0.1448731754347682\n",
      "  batch 3100 loss: 0.1463049154356122\n",
      "  batch 3200 loss: 0.15286753933876754\n",
      "  batch 3300 loss: 0.1344723603129387\n",
      "  batch 3400 loss: 0.12815596040338278\n",
      "  batch 3500 loss: 0.14813397716730833\n",
      "  batch 3600 loss: 0.13054495755583048\n",
      "  batch 3700 loss: 0.13170436054468154\n",
      "  batch 3800 loss: 0.1382951681315899\n",
      "  batch 3900 loss: 0.13678411569446325\n",
      "  batch 4000 loss: 0.13927868105471133\n",
      "  batch 4100 loss: 0.13222162816673516\n",
      "  batch 4200 loss: 0.1219200225174427\n",
      "  batch 4300 loss: 0.11382991466671229\n",
      "  batch 4400 loss: 0.12696985758841037\n",
      "  batch 4500 loss: 0.12078568678349257\n",
      "  batch 4600 loss: 0.1259363268688321\n",
      "  batch 4700 loss: 0.11008732579648495\n",
      "  batch 4800 loss: 0.13100622829049827\n",
      "  batch 4900 loss: 0.09967557486146689\n",
      "  batch 5000 loss: 0.11574848022311926\n",
      "  batch 5100 loss: 0.11083153828978538\n",
      "  batch 5200 loss: 0.11752220679074526\n",
      "  batch 5300 loss: 0.11660331301391125\n",
      "  batch 5400 loss: 0.114621741771698\n",
      "  batch 5500 loss: 0.11839979339390994\n",
      "  batch 5600 loss: 0.10554990697652102\n",
      "  batch 5700 loss: 0.12122773971408606\n",
      "  batch 5800 loss: 0.11191369757056237\n",
      "  batch 5900 loss: 0.10247062653303146\n",
      "  batch 6000 loss: 0.1051609068363905\n",
      "  batch 6100 loss: 0.1139786446467042\n",
      "  batch 6200 loss: 0.11125200498849154\n",
      "  batch 6300 loss: 0.11520799603313207\n",
      "  batch 6400 loss: 0.1098784001171589\n",
      "  batch 6500 loss: 0.10587291166186333\n",
      "  batch 6600 loss: 0.09142053853720426\n",
      "  batch 6700 loss: 0.11161348294466734\n",
      "  batch 6800 loss: 0.11540630791336298\n",
      "  batch 6900 loss: 0.09762386444956064\n",
      "  batch 7000 loss: 0.11808401461690664\n",
      "  batch 7100 loss: 0.1052367103472352\n",
      "  batch 7200 loss: 0.10270257469266653\n",
      "  batch 7300 loss: 0.10454806603491307\n",
      "  batch 7400 loss: 0.11521253041923046\n",
      "  batch 7500 loss: 0.09916731648147106\n",
      "  batch 7600 loss: 0.10797595113515854\n",
      "  batch 7700 loss: 0.10659427125006914\n",
      "  batch 7800 loss: 0.08739159252494573\n",
      "  batch 7900 loss: 0.10813427876681089\n",
      "  batch 8000 loss: 0.10343490339815617\n",
      "  batch 8100 loss: 0.10400523267686367\n",
      "  batch 8200 loss: 0.11764325018972159\n",
      "  batch 8300 loss: 0.09423332884907723\n",
      "  batch 8400 loss: 0.09277395501732827\n",
      "  batch 8500 loss: 0.08628717765212059\n",
      "  batch 8600 loss: 0.10053206715732813\n",
      "  batch 8700 loss: 0.10083324175328016\n",
      "  batch 8800 loss: 0.08859982166439295\n",
      "LOSS train 0.08859982166439295 valid 0.06735402345657349\n",
      "EPOCH 2:\n",
      "  batch 100 loss: 0.08834396630525589\n",
      "  batch 200 loss: 0.098289690092206\n",
      "  batch 300 loss: 0.09807406056672335\n",
      "  batch 400 loss: 0.1012874050065875\n",
      "  batch 500 loss: 0.09406338967382907\n",
      "  batch 600 loss: 0.09223634287714959\n",
      "  batch 700 loss: 0.08983761008828878\n",
      "  batch 800 loss: 0.098571708612144\n",
      "  batch 900 loss: 0.07950675804167986\n",
      "  batch 1000 loss: 0.10617434460669756\n",
      "  batch 1100 loss: 0.08805582877248526\n",
      "  batch 1200 loss: 0.07908320374786854\n",
      "  batch 1300 loss: 0.07694787990301848\n",
      "  batch 1400 loss: 0.09454618982970714\n",
      "  batch 1500 loss: 0.07805932864546776\n",
      "  batch 1600 loss: 0.0864993504807353\n",
      "  batch 1700 loss: 0.08192282170057297\n",
      "  batch 1800 loss: 0.0836440171673894\n",
      "  batch 1900 loss: 0.08899524845182896\n",
      "  batch 2000 loss: 0.0980631522834301\n",
      "  batch 2100 loss: 0.07965594608336687\n",
      "  batch 2200 loss: 0.08692117240279913\n",
      "  batch 2300 loss: 0.09432610094547272\n",
      "  batch 2400 loss: 0.08346107378602027\n",
      "  batch 2500 loss: 0.0895883147791028\n",
      "  batch 2600 loss: 0.08202633790671826\n",
      "  batch 2700 loss: 0.09249746985733509\n",
      "  batch 2800 loss: 0.09733939133584499\n",
      "  batch 2900 loss: 0.10004468612372876\n",
      "  batch 3000 loss: 0.08402060769498348\n",
      "  batch 3100 loss: 0.08873082961887122\n",
      "  batch 3200 loss: 0.08901011262089015\n",
      "  batch 3300 loss: 0.070691341124475\n",
      "  batch 3400 loss: 0.07405894216150045\n",
      "  batch 3500 loss: 0.07169542923569679\n",
      "  batch 3600 loss: 0.07984913311898709\n",
      "  batch 3700 loss: 0.07932854741811753\n",
      "  batch 3800 loss: 0.07406998619437218\n",
      "  batch 3900 loss: 0.07644967909902334\n",
      "  batch 4000 loss: 0.0784012984111905\n",
      "  batch 4100 loss: 0.06896812375634909\n",
      "  batch 4200 loss: 0.07345999661833048\n",
      "  batch 4300 loss: 0.08452465791255236\n",
      "  batch 4400 loss: 0.07895653165876865\n",
      "  batch 4500 loss: 0.08099816847592592\n",
      "  batch 4600 loss: 0.0874221695587039\n",
      "  batch 4700 loss: 0.08380693521350623\n",
      "  batch 4800 loss: 0.08136991076171399\n",
      "  batch 4900 loss: 0.08019391927868127\n",
      "  batch 5000 loss: 0.07266637649387121\n",
      "  batch 5100 loss: 0.07418962202966213\n",
      "  batch 5200 loss: 0.08700425911694765\n",
      "  batch 5300 loss: 0.07434126537293195\n",
      "  batch 5400 loss: 0.09107279039919376\n",
      "  batch 5500 loss: 0.07210970040410757\n",
      "  batch 5600 loss: 0.07750851016491651\n",
      "  batch 5700 loss: 0.07913396246731282\n",
      "  batch 5800 loss: 0.07474167946726083\n",
      "  batch 5900 loss: 0.07646818608045577\n",
      "  batch 6000 loss: 0.061574624218046665\n",
      "  batch 6100 loss: 0.06776299856603146\n",
      "  batch 6200 loss: 0.07008845429867507\n",
      "  batch 6300 loss: 0.0802250037714839\n",
      "  batch 6400 loss: 0.08989416424185037\n",
      "  batch 6500 loss: 0.0697313017398119\n",
      "  batch 6600 loss: 0.07801311638206243\n",
      "  batch 6700 loss: 0.0842887570336461\n",
      "  batch 6800 loss: 0.08328513234853745\n",
      "  batch 6900 loss: 0.07334117975085974\n",
      "  batch 7000 loss: 0.08077184669673443\n",
      "  batch 7100 loss: 0.07765955086797476\n",
      "  batch 7200 loss: 0.07587081383913756\n",
      "  batch 7300 loss: 0.07140450295060873\n",
      "  batch 7400 loss: 0.07136352360248566\n",
      "  batch 7500 loss: 0.0746552349627018\n",
      "  batch 7600 loss: 0.08016277551651001\n",
      "  batch 7700 loss: 0.0843704418092966\n",
      "  batch 7800 loss: 0.06989295441657305\n",
      "  batch 7900 loss: 0.0690003952011466\n",
      "  batch 8000 loss: 0.06233493007719517\n",
      "  batch 8100 loss: 0.07573901195079089\n",
      "  batch 8200 loss: 0.07113380894064904\n",
      "  batch 8300 loss: 0.07622899979352951\n",
      "  batch 8400 loss: 0.07426057398319244\n",
      "  batch 8500 loss: 0.06837187375873327\n",
      "  batch 8600 loss: 0.0784904533997178\n",
      "  batch 8700 loss: 0.078852319419384\n",
      "  batch 8800 loss: 0.07091937765479088\n",
      "LOSS train 0.07091937765479088 valid 0.04806603491306305\n",
      "EPOCH 3:\n",
      "  batch 100 loss: 0.06664088942110538\n",
      "  batch 200 loss: 0.06315050642937421\n",
      "  batch 300 loss: 0.06682924553751945\n",
      "  batch 400 loss: 0.07820322100073099\n",
      "  batch 500 loss: 0.05674693096429109\n",
      "  batch 600 loss: 0.06049380149692297\n",
      "  batch 700 loss: 0.06448377851396798\n",
      "  batch 800 loss: 0.06039001915603876\n",
      "  batch 900 loss: 0.05894192356616259\n",
      "  batch 1000 loss: 0.07151507019996643\n",
      "  batch 1100 loss: 0.06599458076059818\n",
      "  batch 1200 loss: 0.0716915985941887\n",
      "  batch 1300 loss: 0.05731243122369051\n",
      "  batch 1400 loss: 0.06933213874697686\n",
      "  batch 1500 loss: 0.08229520101100206\n",
      "  batch 1600 loss: 0.06959622971713543\n",
      "  batch 1700 loss: 0.07171985168009996\n",
      "  batch 1800 loss: 0.0705531021207571\n",
      "  batch 1900 loss: 0.06454950358718634\n",
      "  batch 2000 loss: 0.0461432920768857\n",
      "  batch 2100 loss: 0.06605999305844307\n",
      "  batch 2200 loss: 0.08328373096883297\n",
      "  batch 2300 loss: 0.07536173727363348\n",
      "  batch 2400 loss: 0.0710992056503892\n",
      "  batch 2500 loss: 0.06429843232035637\n",
      "  batch 2600 loss: 0.0750128949061036\n",
      "  batch 2700 loss: 0.059733260907232764\n",
      "  batch 2800 loss: 0.07390135195106268\n",
      "  batch 2900 loss: 0.05310203589498997\n",
      "  batch 3000 loss: 0.059639384187757966\n",
      "  batch 3100 loss: 0.060389170050621034\n",
      "  batch 3200 loss: 0.06149231355637312\n",
      "  batch 3300 loss: 0.06710371732711792\n",
      "  batch 3400 loss: 0.06160369921475649\n",
      "  batch 3500 loss: 0.061077131070196626\n",
      "  batch 3600 loss: 0.07981103774160146\n",
      "  batch 3700 loss: 0.06816751655191183\n",
      "  batch 3800 loss: 0.07388842955231667\n",
      "  batch 3900 loss: 0.0693377272039652\n",
      "  batch 4000 loss: 0.073448442555964\n",
      "  batch 4100 loss: 0.06230477649718523\n",
      "  batch 4200 loss: 0.0635473781079054\n",
      "  batch 4300 loss: 0.0713400674238801\n",
      "  batch 4400 loss: 0.06939654167741537\n",
      "  batch 4500 loss: 0.06967050559818745\n",
      "  batch 4600 loss: 0.07421806216239929\n",
      "  batch 4700 loss: 0.06784727524966001\n",
      "  batch 4800 loss: 0.06595079265534878\n",
      "  batch 4900 loss: 0.06591245017945767\n",
      "  batch 5000 loss: 0.05966830704361201\n",
      "  batch 5100 loss: 0.056335117854177955\n",
      "  batch 5200 loss: 0.06974294643849134\n",
      "  batch 5300 loss: 0.06499431323260069\n",
      "  batch 5400 loss: 0.07278441198170185\n",
      "  batch 5500 loss: 0.06376405246555805\n",
      "  batch 5600 loss: 0.06778924226760864\n",
      "  batch 5700 loss: 0.0785261844471097\n",
      "  batch 5800 loss: 0.07078397382050752\n",
      "  batch 5900 loss: 0.07023930799216033\n",
      "  batch 6000 loss: 0.06823844786733389\n",
      "  batch 6100 loss: 0.05735376644879579\n",
      "  batch 6200 loss: 0.06457526437938213\n",
      "  batch 6300 loss: 0.056670176833868026\n",
      "  batch 6400 loss: 0.07076681494712829\n",
      "  batch 6500 loss: 0.06818300761282443\n",
      "  batch 6600 loss: 0.07893059767782688\n",
      "  batch 6700 loss: 0.07202723789960146\n",
      "  batch 6800 loss: 0.07240724772214889\n",
      "  batch 6900 loss: 0.05611928202211857\n",
      "  batch 7000 loss: 0.07309955790638924\n",
      "  batch 7100 loss: 0.059860280342400075\n",
      "  batch 7200 loss: 0.05498191997408867\n",
      "  batch 7300 loss: 0.060112410821020604\n",
      "  batch 7400 loss: 0.0648725252598524\n",
      "  batch 7500 loss: 0.05535005871206522\n",
      "  batch 7600 loss: 0.0655965081229806\n",
      "  batch 7700 loss: 0.06791367318481206\n",
      "  batch 7800 loss: 0.06552702900022268\n",
      "  batch 7900 loss: 0.07304435394704342\n",
      "  batch 8000 loss: 0.06939341265708208\n",
      "  batch 8100 loss: 0.0559320068359375\n",
      "  batch 8200 loss: 0.06058689311146736\n",
      "  batch 8300 loss: 0.05665640003979206\n",
      "  batch 8400 loss: 0.06739531517028809\n",
      "  batch 8500 loss: 0.06240345913916826\n",
      "  batch 8600 loss: 0.0649346649646759\n",
      "  batch 8700 loss: 0.06716415613889694\n",
      "  batch 8800 loss: 0.05770604051649571\n",
      "LOSS train 0.05770604051649571 valid 0.039712823927402496\n",
      "EPOCH 4:\n",
      "  batch 100 loss: 0.051165972650051114\n",
      "  batch 200 loss: 0.05930583160370588\n",
      "  batch 300 loss: 0.0742894495278597\n",
      "  batch 400 loss: 0.07167718429118394\n",
      "  batch 500 loss: 0.06242800809442997\n",
      "  batch 600 loss: 0.04796174068003893\n",
      "  batch 700 loss: 0.06699722457677126\n",
      "  batch 800 loss: 0.07190429765731096\n",
      "  batch 900 loss: 0.07517140466719865\n",
      "  batch 1000 loss: 0.05419312592595816\n",
      "  batch 1100 loss: 0.06290347140282393\n",
      "  batch 1200 loss: 0.05227568130940199\n",
      "  batch 1300 loss: 0.05522546663880348\n",
      "  batch 1400 loss: 0.06813590835779905\n",
      "  batch 1500 loss: 0.05803046029061079\n",
      "  batch 1600 loss: 0.0487724320590496\n",
      "  batch 1700 loss: 0.05994788557291031\n",
      "  batch 1800 loss: 0.05967278979718685\n",
      "  batch 1900 loss: 0.060933920405805114\n",
      "  batch 2000 loss: 0.05236902952194214\n",
      "  batch 2100 loss: 0.05405161924660206\n",
      "  batch 2200 loss: 0.05311841126531362\n",
      "  batch 2300 loss: 0.04902475878596306\n",
      "  batch 2400 loss: 0.04230299338698387\n",
      "  batch 2500 loss: 0.060399542339146135\n",
      "  batch 2600 loss: 0.06565191484987735\n",
      "  batch 2700 loss: 0.05602702859789133\n",
      "  batch 2800 loss: 0.04568725742399692\n",
      "  batch 2900 loss: 0.06372405502945185\n",
      "  batch 3000 loss: 0.04730121865868568\n",
      "  batch 3100 loss: 0.0532709526643157\n",
      "  batch 3200 loss: 0.05641784053295851\n",
      "  batch 3300 loss: 0.05430219020694494\n",
      "  batch 3400 loss: 0.05690464287996292\n",
      "  batch 3500 loss: 0.05604507453739643\n",
      "  batch 3600 loss: 0.060847373567521575\n",
      "  batch 3700 loss: 0.061790967807173726\n",
      "  batch 3800 loss: 0.06521290432661772\n",
      "  batch 3900 loss: 0.05196054451167584\n",
      "  batch 4000 loss: 0.07002556741237641\n",
      "  batch 4100 loss: 0.06270040832459926\n",
      "  batch 4200 loss: 0.060702731274068354\n",
      "  batch 4300 loss: 0.05863057438284159\n",
      "  batch 4400 loss: 0.05947369895875454\n",
      "  batch 4500 loss: 0.057551764622330664\n",
      "  batch 4600 loss: 0.04718573987483978\n",
      "  batch 4700 loss: 0.05913297962397337\n",
      "  batch 4800 loss: 0.0558342831581831\n",
      "  batch 4900 loss: 0.05910803586244583\n",
      "  batch 5000 loss: 0.05686104107648134\n",
      "  batch 5100 loss: 0.05749395925551653\n",
      "  batch 5200 loss: 0.057259917445480826\n",
      "  batch 5300 loss: 0.05998093921691179\n",
      "  batch 5400 loss: 0.04999336954206228\n",
      "  batch 5500 loss: 0.04336162775754929\n",
      "  batch 5600 loss: 0.07049791753292084\n",
      "  batch 5700 loss: 0.07756185960024595\n",
      "  batch 5800 loss: 0.05800967436283827\n",
      "  batch 5900 loss: 0.055908446200191975\n",
      "  batch 6000 loss: 0.06675379037857056\n",
      "  batch 6100 loss: 0.07010259222239255\n",
      "  batch 6200 loss: 0.05377813789993525\n",
      "  batch 6300 loss: 0.06130967170000076\n",
      "  batch 6400 loss: 0.053055129684507846\n",
      "  batch 6500 loss: 0.06400122761726379\n",
      "  batch 6600 loss: 0.057415910959243775\n",
      "  batch 6700 loss: 0.05459810711443424\n",
      "  batch 6800 loss: 0.05645507548004389\n",
      "  batch 6900 loss: 0.06375306565314531\n",
      "  batch 7000 loss: 0.07867921866476536\n",
      "  batch 7100 loss: 0.047727455124258994\n",
      "  batch 7200 loss: 0.05059631653130055\n",
      "  batch 7300 loss: 0.059914721660315994\n",
      "  batch 7400 loss: 0.046634773202240465\n",
      "  batch 7500 loss: 0.052669029384851455\n",
      "  batch 7600 loss: 0.054849292561411855\n",
      "  batch 7700 loss: 0.04728614937514067\n",
      "  batch 7800 loss: 0.05701555222272873\n"
     ]
    }
   ],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/emfcamdat_cefr_{}'.format(timestamp))\n",
    "\n",
    "validation_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "EPOCHS = 30\n",
    "best_vloss = 1_000_000.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch + 1))\n",
    "\n",
    "    model.train(True)\n",
    "    avg_loss = train_step(epoch, writer)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, validation_data in enumerate(validation_loader):\n",
    "            validation_inputs, validation_labels = validation_data\n",
    "\n",
    "            input_ids = validation_inputs[\"input_ids\"].to(device)\n",
    "            attention_mask = validation_inputs[\"attention_mask\"].to(device)\n",
    "            validation_labels = validation_labels.to(device)\n",
    "         \n",
    "            with torch.cuda.amp.autocast():\n",
    "                validation_logits = model(input_ids, attention_mask)\n",
    "                validation_loss = loss_fn(validation_logits, validation_labels)\n",
    "                \n",
    "            running_vloss += validation_loss\n",
    "\n",
    "    avg_vloss = running_vloss / len(validation_loader)\n",
    "    print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'runs/model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict(), model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "422f065d-1ae0-4e12-bbad-bd7430a84bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42/4153173265.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs[\"input_ids\"] = torch.squeeze(torch.tensor(tokenized[\"input_ids\"], dtype=torch.long))\n",
      "/tmp/ipykernel_42/4153173265.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs[\"attention_mask\"] = torch.squeeze(torch.tensor(tokenized[\"attention_mask\"], dtype=torch.long))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss per 100 steps: 0.061803024262189865\n",
      "Validation Accuracy per 100 steps: 96.875\n",
      "Validation Loss per 100 steps: 0.03677838033734876\n",
      "Validation Accuracy per 100 steps: 98.66955445544555\n",
      "Validation Loss per 100 steps: 0.03475033012824017\n",
      "Validation Accuracy per 100 steps: 98.75621890547264\n",
      "Validation Loss per 100 steps: 0.03642535056720451\n",
      "Validation Accuracy per 100 steps: 98.79568106312293\n",
      "Validation Loss per 100 steps: 0.03509403241739058\n",
      "Validation Accuracy per 100 steps: 98.80766832917706\n",
      "Validation Loss per 100 steps: 0.0352492569752308\n",
      "Validation Accuracy per 100 steps: 98.81487025948104\n",
      "Validation Loss per 100 steps: 0.036752852275799074\n",
      "Validation Accuracy per 100 steps: 98.74168053244593\n",
      "Validation Loss per 100 steps: 0.03810991224944598\n",
      "Validation Accuracy per 100 steps: 98.6938302425107\n",
      "Validation Loss per 100 steps: 0.03809177700826781\n",
      "Validation Accuracy per 100 steps: 98.65792759051186\n",
      "Validation Loss per 100 steps: 0.03843660812986344\n",
      "Validation Accuracy per 100 steps: 98.65774139844618\n",
      "Validation Loss per 100 steps: 0.03930843935757059\n",
      "Validation Accuracy per 100 steps: 98.6575924075924\n",
      "Validation Loss per 100 steps: 0.04009296066061051\n",
      "Validation Accuracy per 100 steps: 98.62908719346049\n",
      "Validation Loss per 100 steps: 0.03978332901183713\n",
      "Validation Accuracy per 100 steps: 98.65216486261448\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nb_tr_steps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_accu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epoch_accu\n\u001b[0;32m---> 52\u001b[0m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 45\u001b[0m, in \u001b[0;36mvalid\u001b[0;34m(model, testing_loader)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss per 100 steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy per 100 steps: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccu_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m total\u001b[38;5;241m/\u001b[39m\u001b[43mnb_tr_steps\u001b[49m\n\u001b[1;32m     46\u001b[0m epoch_accu \u001b[38;5;241m=\u001b[39m (n_correct\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m/\u001b[39mnb_tr_examples\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Loss Epoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nb_tr_steps' is not defined"
     ]
    }
   ],
   "source": [
    "eval_model = CEFRClassifier(5)\n",
    "eval_model.load_state_dict(torch.load(\"runs/model_20240620_204416_2\"))\n",
    "\n",
    "bs = 32\n",
    "test_loader = DataLoader(test_set, batch_size=bs, shuffle=True)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "def compute_accuracy(big_idx, targets):\n",
    "    return (big_idx==targets).sum().item()\n",
    "\n",
    "def validate(model, testing_loader):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    n_correct = 0\n",
    "    n_wrong = 0\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    nb_tr_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testing_loader):\n",
    "            test_inputs, test_labels = data\n",
    "\n",
    "            ids = test_inputs['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = test_inputs['attention_mask'].to(device, dtype = torch.long)\n",
    "            targets = test_labels.to(device, dtype = torch.long)\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "            \n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            big_val, big_idx = torch.max(outputs.data, dim=1)\n",
    "            n_correct += compute_accuracy(big_idx, targets)\n",
    "\n",
    "            nb_tr_examples+=targets.size(0)\n",
    "            \n",
    "            if i%100==0:\n",
    "                loss_step = total_loss/(i+1)\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation Loss per 100 steps: {loss_step}\")\n",
    "                print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n",
    "                \n",
    "    epoch_loss = total/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n",
    "    \n",
    "    return epoch_accu\n",
    "\n",
    "validate(eval_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195c533-7a72-49d8-a0fc-9d79b23092ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b1569-a4b7-4d5c-b7c4-58a03d66297e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
